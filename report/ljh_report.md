---
title: "ljhreport"
author: 蓝俊皓
date: May 22, 2020
output:
  word_document:
    path: C:\Users\41409\Desktop\bc\ljhreport.docx
---

**model overview**

For a classification problem to predict the probability of individuals, the first came into one's head is the logistic regression model. It is a generalized linear model, so it is less likely to become overfitting than a nonlinear model. And it has some perfect properties that is convenient for us to make regression diagnosis. However, the input features are not that complete, which means some features have many missing values, and for some other features, the proportion of having zero values is more than 90%. Due to the inperfect condition of our data, we also consider the gradient boosting desition tree. Specifically, we use the lightGBM algorithm. 

**logistic regression**

For the logistic regression model, we use pytorch to realize it. First, let all the input features $X_{n,p}$ pass through a fully connected layer and the output of this layer $X_{n,p}^T\beta$ is one-dimensional, where $\beta$ is the parameters to be estimated. Then let the output of the first layer pass through an active function, which is the sigmoid function here. Then, the output is the probability of each individual. As for some technical details, we use ADAM as the optimizer and reduce the learning rate dynamically by monitoring the evaluation loss.

Before training, we split the dataset to make 20% of data the evaluation dataset. Since the training data is siginificantly unbalanced, we spent a little trick on choosing the epoch train data. Now let's denote the total number of vaccined samples as $n_0$ and the number of unvaccined sample as $n_1$. In every epoch of training, we first used all the vaccined samples, whose sample size was $n_0$, as the training data. Then, we also chose $n_0$ from the unvaccined samples randomly. In this case, we could make sure that the training data for each epoch was strictly balanced and the total sample size was $2n_0$. If we train for enough epoches, we should utilize all the information from the original dataset.


**logistic regression model diagnosis**

Referring to tensorboard, we could visualize the training procss. Obviously the auc on the evaluation dataset increases smoothly, that means the over-fitting situation doesn't happen. The next step is to calculate the standard deviation of the estimated parameters, which is given by
  $Var(\hat{\beta})=\left( X_{n,p}^TVX_{n,p} \right)^{-1}$
where $V$ is the diagonal matrix generated by $\{\hat{p_i}(1-\hat{p_i}), i=1,2,...,n\}$. The rule of feature selecting is that, if $0$ falls into such an interval $\left[ \hat{\beta_j} - 2sd(\hat{\beta_j}), \hat{\beta_j} + 2sd(\hat{\beta_j}) \right]$, then $\beta_j$ is insignificant. After this step, we have about 300 variables which are significant. What worth mentioning is that, though some features have a large $\beta$, they are not significant due to a relatively larger standard deviation. By the way, there is no need to do Lasso logistic regression to enforce some $\beta_j$ to $0$, since our sample size is greatly larger than the number of variables.

**bosting tree**

Boosting tree is a set of nonlinear decision tree models. To make a brief introduction, after generating a single tree $\Theta_i(X)$, we will generate a new tree $\Theta_{i+1}(X)$ to fit the regression residuals. So the predicting model is given by
$\mathbb{F}=\sum_{i=1}^n \Theta_i(X)$
We use lightgbm here because it is fast and precise as well. To avoid over-fitting, we choose the maximum depth of a tree ti be 6, maximum number of leaves in a single tree to be 60. This can not be too small since we have about 800 input features and should be less than $2^6$. What's more, we set the minimum data in every leaf to 1000, and number of trees to 300.

Similarly, we split the dataset to make 20% of data the evaluation dataset. As mentioned before, our data is unbalanced, and we can not use the batch-training method in lightgbm, since trees are not additive. So, using the prior notation $n_0$ and $n_1$, we choose ramdomly 4 subsamples from unvaccniated samples and the size of each subsample is $n_1$. Then we concatenate the vaccinated sample with these subsamples respectively to get 4 training datasets. On each dataset, we train the lightgbm model with the same hyperparameters. In the end, for evaluation and holdout data, we use these 4 fully trained lightgbm models to predict the probability of each individual and average them as the final predicted probability.


model blending
